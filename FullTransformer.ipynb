{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0a4c7610",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import *\n",
    "import keras.backend as K\n",
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0befdda3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 位置编码\n",
    "class PositionEncoding(Layer):\n",
    "\n",
    "    def __init__(self, model_dim, **kwargs):\n",
    "        self._model_dim = model_dim\n",
    "        super(PositionEncoding, self).__init__(**kwargs)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # 获取输入序列的向量数量，序列长度\n",
    "        seq_length = inputs.shape[1]\n",
    "        position_encodings = np.zeros((seq_length, self._model_dim))\n",
    "        for pos in range(seq_length):\n",
    "            for i in range(self._model_dim):\n",
    "                position_encodings[pos, i] = pos / np.power(10000, (i-i%2) / self._model_dim)\n",
    "        position_encodings[:, 0::2] = np.sin(position_encodings[:, 0::2]) # 2i\n",
    "        position_encodings[:, 1::2] = np.cos(position_encodings[:, 1::2]) # 2i+1\n",
    "        position_encodings = K.cast(position_encodings, 'float32')\n",
    "        return position_encodings\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "edfe8e06",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embedding(Layer):\n",
    "\n",
    "    def __init__(self, vocab_size, model_dim, **kwargs):\n",
    "        self._vocab_size = vocab_size\n",
    "        self._model_dim = model_dim\n",
    "        super(Embedding, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.embeddings = self.add_weight(\n",
    "            shape=(self._vocab_size, self._model_dim),\n",
    "            initializer='glorot_uniform',\n",
    "            name=\"embeddings\")\n",
    "        super(Embedding, self).build(input_shape)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        if K.dtype(inputs) != 'int32':\n",
    "            inputs = K.cast(inputs, 'int32')\n",
    "        embeddings = K.gather(self.embeddings, inputs)\n",
    "        embeddings *= self._model_dim ** 0.5 # Scale\n",
    "        return embeddings\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "\n",
    "        return input_shape + (self._model_dim,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c8678c37",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Add(Layer):\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        super(Add, self).__init__(**kwargs)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        input_a, input_b = inputs\n",
    "        return input_a + input_b\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "23bda094",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScaledDotProductAttention(Layer):\n",
    "\n",
    "    def __init__(self, masking=True, future=False, dropout_rate=0., **kwargs):\n",
    "        self._masking = masking\n",
    "        self._future = future\n",
    "        self._dropout_rate = dropout_rate\n",
    "        self._masking_num = -2**32+1\n",
    "        super(ScaledDotProductAttention, self).__init__(**kwargs)\n",
    "\n",
    "    def mask(self, inputs, masks):\n",
    "        masks = K.cast(masks, 'float32')\n",
    "        masks = K.tile(masks, [K.shape(inputs)[0] // K.shape(masks)[0], 1])\n",
    "        masks = K.expand_dims(masks, 1)\n",
    "        outputs = inputs + masks * self._masking_num\n",
    "        return outputs\n",
    "    \n",
    "    def future_mask(self, inputs):\n",
    "        diag_vals = tf.ones_like(inputs[0, :, :])\n",
    "        tril = tf.linalg.LinearOperatorLowerTriangular(diag_vals).to_dense()  \n",
    "        future_masks = tf.tile(tf.expand_dims(tril, 0), [tf.shape(inputs)[0], 1, 1])\n",
    "        paddings = tf.ones_like(future_masks) * self._masking_num\n",
    "        outputs = tf.where(tf.equal(future_masks, 0), paddings, inputs)\n",
    "        return outputs\n",
    "\n",
    "    def call(self, inputs):\n",
    "        if self._masking:\n",
    "            assert len(inputs) == 4, \"inputs should be set [queries, keys, values, masks].\"\n",
    "            queries, keys, values, masks = inputs\n",
    "        else:\n",
    "            assert len(inputs) == 3, \"inputs should be set [queries, keys, values].\"\n",
    "            queries, keys, values = inputs\n",
    "\n",
    "        if K.dtype(queries) != 'float32':  queries = K.cast(queries, 'float32')\n",
    "        if K.dtype(keys) != 'float32':  keys = K.cast(keys, 'float32')\n",
    "        if K.dtype(values) != 'float32':  values = K.cast(values, 'float32')\n",
    "\n",
    "        matmul = K.batch_dot(queries, tf.transpose(keys, [0, 2, 1])) # MatMul\n",
    "        scaled_matmul = matmul / int(queries.shape[-1]) ** 0.5  # Scale\n",
    "        if self._masking:\n",
    "            scaled_matmul = self.mask(scaled_matmul, masks) # Mask(opt.)\n",
    "\n",
    "        if self._future:\n",
    "            scaled_matmul = self.future_mask(scaled_matmul)\n",
    "\n",
    "        softmax_out = K.softmax(scaled_matmul) # SoftMax\n",
    "        # Dropout\n",
    "        out = K.dropout(softmax_out, self._dropout_rate)\n",
    "        \n",
    "        outputs = K.batch_dot(out, values)\n",
    "\n",
    "        return outputs\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4cec8adc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(Layer):\n",
    "\n",
    "    def __init__(self, n_heads, head_dim, dropout_rate=.1, masking=True, future=False, trainable=True, **kwargs):\n",
    "        self._n_heads = n_heads\n",
    "        self._head_dim = head_dim\n",
    "        self._dropout_rate = dropout_rate\n",
    "        self._masking = masking\n",
    "        self._future = future\n",
    "        self._trainable = trainable\n",
    "        super(MultiHeadAttention, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self._weights_queries = self.add_weight(\n",
    "            shape=(input_shape[0][-1], self._n_heads * self._head_dim),\n",
    "            initializer='glorot_uniform',\n",
    "            trainable=self._trainable,\n",
    "            name='weights_queries')\n",
    "        self._weights_keys = self.add_weight(\n",
    "            shape=(input_shape[1][-1], self._n_heads * self._head_dim),\n",
    "            initializer='glorot_uniform',\n",
    "            trainable=self._trainable,\n",
    "            name='weights_keys')\n",
    "        self._weights_values = self.add_weight(\n",
    "            shape=(input_shape[2][-1], self._n_heads * self._head_dim),\n",
    "            initializer='glorot_uniform',\n",
    "            trainable=self._trainable,\n",
    "            name='weights_values')\n",
    "        super(MultiHeadAttention, self).build(input_shape)\n",
    "\n",
    "\n",
    "    def call(self, inputs):\n",
    "        if self._masking:\n",
    "            assert len(inputs) == 4, \"inputs should be set [queries, keys, values, masks].\"\n",
    "            queries, keys, values, masks = inputs\n",
    "        else:\n",
    "            assert len(inputs) == 3, \"inputs should be set [queries, keys, values].\"\n",
    "            queries, keys, values = inputs\n",
    "        \n",
    "        queries_linear = K.dot(queries, self._weights_queries) \n",
    "        keys_linear = K.dot(keys, self._weights_keys)\n",
    "        values_linear = K.dot(values, self._weights_values)\n",
    "\n",
    "        queries_multi_heads = tf.concat(tf.split(queries_linear, self._n_heads, axis=2), axis=0)\n",
    "        keys_multi_heads = tf.concat(tf.split(keys_linear, self._n_heads, axis=2), axis=0)\n",
    "        values_multi_heads = tf.concat(tf.split(values_linear, self._n_heads, axis=2), axis=0)\n",
    "        \n",
    "        if self._masking:\n",
    "            att_inputs = [queries_multi_heads, keys_multi_heads, values_multi_heads, masks]\n",
    "        else:\n",
    "            att_inputs = [queries_multi_heads, keys_multi_heads, values_multi_heads]\n",
    "            \n",
    "        attention = ScaledDotProductAttention(\n",
    "            masking=self._masking, future=self._future, dropout_rate=self._dropout_rate)\n",
    "        att_out = attention(att_inputs)\n",
    "\n",
    "        outputs = tf.concat(tf.split(att_out, self._n_heads, axis=0), axis=2)\n",
    "        \n",
    "        return outputs\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e4160a06",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionWiseFeedForward(Layer):\n",
    "    \n",
    "    def __init__(self, model_dim, inner_dim, trainable=True, **kwargs):\n",
    "        self._model_dim = model_dim\n",
    "        self._inner_dim = inner_dim\n",
    "        self._trainable = trainable\n",
    "        super(PositionWiseFeedForward, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.weights_inner = self.add_weight(\n",
    "            shape=(input_shape[-1], self._inner_dim),\n",
    "            initializer='glorot_uniform',\n",
    "            trainable=self._trainable,\n",
    "            name=\"weights_inner\")\n",
    "        self.weights_out = self.add_weight(\n",
    "            shape=(self._inner_dim, self._model_dim),\n",
    "            initializer='glorot_uniform',\n",
    "            trainable=self._trainable,\n",
    "            name=\"weights_out\")\n",
    "        self.bais_inner = self.add_weight(\n",
    "            shape=(self._inner_dim,),\n",
    "            initializer='uniform',\n",
    "            trainable=self._trainable,\n",
    "            name=\"bais_inner\")\n",
    "        self.bais_out = self.add_weight(\n",
    "            shape=(self._model_dim,),\n",
    "            initializer='uniform',\n",
    "            trainable=self._trainable,\n",
    "            name=\"bais_out\")\n",
    "        super(PositionWiseFeedForward, self).build(input_shape)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        if K.dtype(inputs) != 'float32':\n",
    "            inputs = K.cast(inputs, 'float32')\n",
    "        inner_out = K.relu(K.dot(inputs, self.weights_inner) + self.bais_inner)\n",
    "        outputs = K.dot(inner_out, self.weights_out) + self.bais_out\n",
    "        return outputs\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return self._model_dim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "836ecf28",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNormalization(Layer):\n",
    "\n",
    "    def __init__(self, epsilon=1e-8, **kwargs):\n",
    "        self._epsilon = epsilon\n",
    "        super(LayerNormalization, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.beta = self.add_weight(\n",
    "            shape=(input_shape[-1],),\n",
    "            initializer='zero',\n",
    "            name='beta')\n",
    "        self.gamma = self.add_weight(\n",
    "            shape=(input_shape[-1],),\n",
    "            initializer='one',\n",
    "            name='gamma')\n",
    "        super(LayerNormalization, self).build(input_shape)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        mean, variance = tf.nn.moments(inputs, [-1], keepdims=True)\n",
    "        normalized = (inputs - mean) / ((variance + self._epsilon) ** 0.5)\n",
    "        outputs = self.gamma * normalized + self.beta\n",
    "        return outputs\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fc4801a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(Layer):\n",
    "\n",
    "    def __init__(self, vocab_size, model_dim, \n",
    "            n_heads=8, encoder_stack=6, decoder_stack=6, feed_forward_size=2048, dropout_rate=0.1, **kwargs):\n",
    "        self._vocab_size = vocab_size\n",
    "        self._model_dim = model_dim\n",
    "        self._n_heads = n_heads\n",
    "        self._encoder_stack = encoder_stack\n",
    "        self._decoder_stack = decoder_stack\n",
    "        self._feed_forward_size = feed_forward_size\n",
    "        self._dropout_rate = dropout_rate\n",
    "        super(Transformer, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.embeddings = self.add_weight(\n",
    "            shape=(self._vocab_size, self._model_dim),\n",
    "            initializer='glorot_uniform',\n",
    "            trainable=True,\n",
    "            name=\"embeddings\")\n",
    "        super(Transformer, self).build(input_shape)\n",
    "\n",
    "\n",
    "    def encoder(self, inputs):\n",
    "        if K.dtype(inputs) != 'int32':\n",
    "            inputs = K.cast(inputs, 'int32')\n",
    "\n",
    "        masks = K.equal(inputs, 0)\n",
    "        # Embeddings\n",
    "        embeddings = K.gather(self.embeddings, inputs)\n",
    "        embeddings *= self._model_dim ** 0.5 # Scale\n",
    "        # Position Encodings\n",
    "        position_encodings = PositionEncoding(self._model_dim)(embeddings)\n",
    "        # Embedings + Postion-encodings\n",
    "        encodings = embeddings + position_encodings\n",
    "        # Dropout\n",
    "        encodings = K.dropout(encodings, self._dropout_rate)\n",
    "\n",
    "        for i in range(self._encoder_stack):\n",
    "            # Multi-head-Attention\n",
    "            attention = MultiHeadAttention(self._n_heads, self._model_dim // self._n_heads)\n",
    "            attention_input = [encodings, encodings, encodings, masks]\n",
    "            attention_out = attention(attention_input)\n",
    "            # Add & Norm\n",
    "            attention_out += encodings\n",
    "            attention_out = LayerNormalization()(attention_out)\n",
    "            # Feed-Forward\n",
    "            ff = PositionWiseFeedForward(self._model_dim, self._feed_forward_size)\n",
    "            ff_out = ff(attention_out)\n",
    "            # Add & Norm\n",
    "            ff_out += attention_out\n",
    "            encodings = LayerNormalization()(ff_out)\n",
    "\n",
    "        return encodings, masks\n",
    "\n",
    "\n",
    "    def decoder(self, inputs):\n",
    "        decoder_inputs, encoder_encodings, encoder_masks = inputs\n",
    "        if K.dtype(decoder_inputs) != 'int32':\n",
    "            decoder_inputs = K.cast(decoder_inputs, 'int32')\n",
    "\n",
    "        decoder_masks = K.equal(decoder_inputs, 0)\n",
    "        # Embeddings\n",
    "        embeddings = K.gather(self.embeddings, decoder_inputs)\n",
    "        embeddings *= self._model_dim ** 0.5 # Scale\n",
    "        # Position Encodings\n",
    "        position_encodings = PositionEncoding(self._model_dim)(embeddings)\n",
    "        # Embedings + Postion-encodings\n",
    "        encodings = embeddings + position_encodings\n",
    "        # Dropout\n",
    "        encodings = K.dropout(encodings, self._dropout_rate)\n",
    "        \n",
    "        for i in range(self._decoder_stack):\n",
    "            # Masked-Multi-head-Attention\n",
    "            masked_attention = MultiHeadAttention(self._n_heads, self._model_dim // self._n_heads, future=True)\n",
    "            masked_attention_input = [encodings, encodings, encodings, decoder_masks]\n",
    "            masked_attention_out = masked_attention(masked_attention_input)\n",
    "            # Add & Norm\n",
    "            masked_attention_out += encodings\n",
    "            masked_attention_out = LayerNormalization()(masked_attention_out)\n",
    "\n",
    "            # Multi-head-Attention\n",
    "            attention = MultiHeadAttention(self._n_heads, self._model_dim // self._n_heads)\n",
    "            attention_input = [masked_attention_out, encoder_encodings, encoder_encodings, encoder_masks]\n",
    "            attention_out = attention(attention_input)\n",
    "            # Add & Norm\n",
    "            attention_out += masked_attention_out\n",
    "            attention_out = LayerNormalization()(attention_out)\n",
    "\n",
    "            # Feed-Forward\n",
    "            ff = PositionWiseFeedForward(self._model_dim, self._feed_forward_size)\n",
    "            ff_out = ff(attention_out)\n",
    "            # Add & Norm\n",
    "            ff_out += attention_out\n",
    "            encodings = LayerNormalization()(ff_out)\n",
    "\n",
    "        # Pre-Softmax 与 Embeddings 共享参数\n",
    "        linear_projection = K.dot(encodings, K.transpose(self.embeddings))\n",
    "        outputs = K.softmax(linear_projection)\n",
    "        return outputs\n",
    "\n",
    "    def call(self, inputs):\n",
    "        encoder_inputs, decoder_inputs = inputs\n",
    "        encoder_encodings, encoder_masks = self.encoder(encoder_inputs)\n",
    "        encoder_outputs = self.decoder([decoder_inputs, encoder_encodings, encoder_masks])\n",
    "        return encoder_outputs\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return  (input_shape[0][0], input_shape[0][1], self._vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "97bf103d",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Exception encountered when calling layer \"transformer_1\" (type Transformer).\n\nin user code:\n\n    File \"/var/folders/qv/lkzvp5v136j4x83fvc1qbgs00000gn/T/ipykernel_22649/3359994373.py\", line 103, in call  *\n        encoder_encodings, encoder_masks = self.encoder(encoder_inputs)\n    File \"/var/folders/qv/lkzvp5v136j4x83fvc1qbgs00000gn/T/ipykernel_22649/3359994373.py\", line 42, in encoder  *\n        attention_out = attention(attention_input)\n    File \"/Users/archer/Documents/jupyter/venv_workspace/lib/python3.8/site-packages/keras/utils/traceback_utils.py\", line 70, in error_handler  **\n        raise e.with_traceback(filtered_tb) from None\n    File \"/Users/archer/Documents/jupyter/venv_workspace/lib/python3.8/site-packages/keras/utils/traceback_utils.py\", line 96, in error_handler\n        return fn(*args, **kwargs)\n\n    TypeError: call() missing 1 required positional argument: 'value'\n\n\nCall arguments received by layer \"transformer_1\" (type Transformer):\n  • inputs=['tf.Tensor(shape=(None, 256), dtype=float32)', 'tf.Tensor(shape=(None, 256), dtype=float32)']",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 11\u001b[0m\n\u001b[1;32m      9\u001b[0m encoder_inputs \u001b[38;5;241m=\u001b[39m Input(shape\u001b[38;5;241m=\u001b[39m(max_seq_len,), name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoder_inputs\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     10\u001b[0m decoder_inputs \u001b[38;5;241m=\u001b[39m Input(shape\u001b[38;5;241m=\u001b[39m(max_seq_len,), name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdecoder_inputs\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 11\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mTransformer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvocab_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_dim\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mencoder_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecoder_inputs\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m model \u001b[38;5;241m=\u001b[39m Model(inputs\u001b[38;5;241m=\u001b[39m[encoder_inputs, decoder_inputs], outputs\u001b[38;5;241m=\u001b[39moutputs)\n\u001b[1;32m     14\u001b[0m model\u001b[38;5;241m.\u001b[39msummary()\n",
      "File \u001b[0;32m~/Documents/jupyter/venv_workspace/lib/python3.8/site-packages/keras/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/var/folders/qv/lkzvp5v136j4x83fvc1qbgs00000gn/T/__autograph_generated_filenga84e0j.py:11\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m      9\u001b[0m retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mUndefinedReturnValue()\n\u001b[1;32m     10\u001b[0m (encoder_inputs, decoder_inputs) \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mld(inputs)\n\u001b[0;32m---> 11\u001b[0m (encoder_encodings, encoder_masks) \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39mencoder, (ag__\u001b[38;5;241m.\u001b[39mld(encoder_inputs),), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[1;32m     12\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39mdecoder, ([ag__\u001b[38;5;241m.\u001b[39mld(decoder_inputs), ag__\u001b[38;5;241m.\u001b[39mld(encoder_encodings), ag__\u001b[38;5;241m.\u001b[39mld(encoder_masks)],), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/var/folders/qv/lkzvp5v136j4x83fvc1qbgs00000gn/T/__autograph_generated_fileixkex820.py:61\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__encoder\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m     59\u001b[0m attention_out \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mUndefined(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mattention_out\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     60\u001b[0m ff_out \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mUndefined(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mff_out\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 61\u001b[0m ag__\u001b[38;5;241m.\u001b[39mfor_stmt(ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mrange\u001b[39m), (ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m_encoder_stack,), \u001b[38;5;28;01mNone\u001b[39;00m, fscope), \u001b[38;5;28;01mNone\u001b[39;00m, loop_body, get_state_1, set_state_1, (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencodings\u001b[39m\u001b[38;5;124m'\u001b[39m,), {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124miterate_names\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mi\u001b[39m\u001b[38;5;124m'\u001b[39m})\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     63\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/var/folders/qv/lkzvp5v136j4x83fvc1qbgs00000gn/T/__autograph_generated_fileixkex820.py:46\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__encoder.<locals>.loop_body\u001b[0;34m(itr)\u001b[0m\n\u001b[1;32m     44\u001b[0m attention \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(MultiHeadAttention), (ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m_n_heads, (ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m_model_dim \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m_n_heads)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[1;32m     45\u001b[0m attention_input \u001b[38;5;241m=\u001b[39m [ag__\u001b[38;5;241m.\u001b[39mld(encodings), ag__\u001b[38;5;241m.\u001b[39mld(encodings), ag__\u001b[38;5;241m.\u001b[39mld(encodings), ag__\u001b[38;5;241m.\u001b[39mld(masks)]\n\u001b[0;32m---> 46\u001b[0m attention_out \u001b[38;5;241m=\u001b[39m \u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconverted_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43mattention\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43mattention_input\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfscope\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     47\u001b[0m attention_out \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mld(attention_out)\n\u001b[1;32m     48\u001b[0m attention_out \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m encodings\n",
      "\u001b[0;31mTypeError\u001b[0m: Exception encountered when calling layer \"transformer_1\" (type Transformer).\n\nin user code:\n\n    File \"/var/folders/qv/lkzvp5v136j4x83fvc1qbgs00000gn/T/ipykernel_22649/3359994373.py\", line 103, in call  *\n        encoder_encodings, encoder_masks = self.encoder(encoder_inputs)\n    File \"/var/folders/qv/lkzvp5v136j4x83fvc1qbgs00000gn/T/ipykernel_22649/3359994373.py\", line 42, in encoder  *\n        attention_out = attention(attention_input)\n    File \"/Users/archer/Documents/jupyter/venv_workspace/lib/python3.8/site-packages/keras/utils/traceback_utils.py\", line 70, in error_handler  **\n        raise e.with_traceback(filtered_tb) from None\n    File \"/Users/archer/Documents/jupyter/venv_workspace/lib/python3.8/site-packages/keras/utils/traceback_utils.py\", line 96, in error_handler\n        return fn(*args, **kwargs)\n\n    TypeError: call() missing 1 required positional argument: 'value'\n\n\nCall arguments received by layer \"transformer_1\" (type Transformer):\n  • inputs=['tf.Tensor(shape=(None, 256), dtype=float32)', 'tf.Tensor(shape=(None, 256), dtype=float32)']"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.utils import plot_model\n",
    "\n",
    "vocab_size = 5000\n",
    "max_seq_len = 256 \n",
    "model_dim = 512\n",
    "\n",
    "encoder_inputs = Input(shape=(max_seq_len,), name='encoder_inputs')\n",
    "decoder_inputs = Input(shape=(max_seq_len,), name='decoder_inputs')\n",
    "outputs = Transformer(vocab_size, model_dim)([encoder_inputs, decoder_inputs])\n",
    "model = Model(inputs=[encoder_inputs, decoder_inputs], outputs=outputs)\n",
    "\n",
    "model.summary()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_workspace_kernel",
   "language": "python",
   "name": "venv_workspace"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
